{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Feature Coercion and Feature Generation**\n",
    "\n",
    "> We can only feed numbers into our linear regression model, yet often our data is in other forms.  Our goal here is to convert as much of our data as possible into numbers so that we can feed it into our machine learning model.\n",
    "\n",
    "In class, we saw how to coerce the following types of data.\n",
    "\n",
    "* Text to datetimes (eg. time since, months ago)\n",
    "    * Once we have data in date time formats, use `add_datepart` to convert to numbers, and generate additional features\n",
    "* \"almost\" numbers to numbers ($40.00, 25% to 40, 25)\n",
    "* Categorical data\n",
    "    * Coercing text like cities and neighborhoods into categorical\n",
    "    * Combining feature variables into other category observations fall below threshold\n",
    "* Text to booleans (\"T\", \"F\", to True False)\n",
    "    * Identifying data that is *almost* boolean (because dominated by one category) and converting to boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From our data we can generate additional features.\n",
    "\n",
    "1. DateTime - \n",
    "    * `add_datepart` to identify day of week, month, year, etc.\n",
    "    * Can add time since, or before to relevant dates (eg. Christmas, for retailers)\n",
    "    * Time since or after internal dates (eg. Listing since user registration)\n",
    "2. Geographic\n",
    "    * Distance from a location\n",
    "    * Zip Codes - converted to categories, and removed the last digit to limit number of zip codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Feature Coercion and Generation Requirements**\n",
    "\n",
    "1. Basic Requirements\n",
    "    * Convert text to booleans where applicable\n",
    "    * All categorical data to multiple features via get dummies\n",
    "\n",
    "2. Data Munging Requirements\n",
    "For at least three columns, do the following:\n",
    "    * Convert columns that begin as type \"object\" into categories\n",
    "    * Convert columns that begin as type \"object\" into datetimes\n",
    "    * Convert geographic data into numbers (via distance, or zip codes, or categories)\n",
    "    * Convert \"almost\" numbers to numbers \n",
    "3. Clean Code Requirements\n",
    "    * We want our code be reusable, for future data science projects, and within this project\n",
    "    * To that end, **write at least three methods** so that we automate some of the coercion and feature generation\n",
    "        * To this end, think about function argument of a dataframe, a column, or multiple columns, and outputting a new column, a subset of columns, or a coerced column\n",
    "        * Take a function that we wrote in class, change or expand upon the function\n",
    "\n",
    "> The data munging requirements are the most difficult to specify.  Essentially, I am looking to see that you are able to extract data that is not perfectly formatted.  If your data is already pre-formatted, then add an additional dataset via an API, or tie together two datasets.  That will count.  If you have questions, please give email me with a link to your dataset and the data munging work you plan on doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Feature Selection and Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After coercing and generating our feature, we can begin selecting features.  Selecting features allows us to reduce error due to variance, reduce multicollinearity, and increase interpretability of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Recursive Feature Elimination with Cross Validation\n",
    "    * Quickly create multiple models with recursive feature elimination\n",
    "    * Discover the number of features our model can be limited to before a significant drop in $r^2$ score.\n",
    "2. Recursive Feature Elimination\n",
    "    * Once we have discovered the number of features to be selected, we use recursive feature elimination to discover which of those features we should select\n",
    "    * Use `rfe.support_` to identify those features\n",
    "3. (Optional) Plot the target variable \n",
    "    * Can we improve our scores by either eliminating outliers or taking a log of our target variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Correlation Analysis\n",
    "    * In the July 30 class, we will discuss correlation analysis, and see how it can further allow us to prune and combine features.\n",
    "    1. Use \"rank\" scatter plots to identify highly correlated features\n",
    "    2. Use spearman correlations to identify highly correlated features\n",
    "    3. Use a dendrogram to identify highly correlated features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Once you have selected "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
