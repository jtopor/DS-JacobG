{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we were introduced to a random forest.  We saw that with decision trees, we not only get a high degree of variation in the leafs of a tree, but also throughout the tree.  This is because a decision tree works sequentially, a split of a data at a higher level affects how the data will be split at lower levels.  It's also because decision trees happen to have very few assumptions about the data -- unlike say a linear model which assumes that the data fits some form of a line.  With a decision trees,Â we are working with a very flexible model and this gives us a wide degree of variation.\n",
    "\n",
    "This variance is a problem if using just one decision tree to make our predictions.  However, we can reduce our variance by fitting many different trees, and then taking the average of the predictions.  This is the main idea behind a random forest.  In this lesson, we'll begin working with a random forest in sklearn.  As we move through this and subsequent lessons, it's important to remember that the main idea behind a random forest:\n",
    "\n",
    "* Use a highly flexible, and variant, model to find different patterns in the data and then reduce that variance through aggregation. \n",
    "\n",
    "Aggregating the models reduces the effects of random patterns which are unlikely to happen again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, let's work with the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dataset = load_diabetes()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest from the Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first idea of random forests is one that we already know.  With random forests we are averaging the predictions from a number of decision trees.  We can see this directly by loading up our random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=None,\n",
       "           oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators = 25, random_state = 1)\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we fit our random forest regressor, our regressors comes with a new attribute, called `estimators`.  An estimator is a particular tree of a random forest.  Let's take a look at the first two estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1791095845, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=2135392491, splitter='best')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = rfr.estimators_\n",
    "len(estimators)\n",
    "# 25\n",
    "estimators[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each of these estimators is simply one of the decision trees that we saw earlier on.  We can see each of the estimator's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 83.,  93.,  47.,  77.,  96.,  69.,  69.,  69.,  93.,  64.,  96.,\n",
       "        88., 135.,  77.,  96.,  60.,  37.,  31., 116., 125.,  64.,  77.,\n",
       "        96.,  96.,  47.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tree_predictions = np.hstack([estimator.predict(X_test[:1, :]) for estimator in rfr.estimators_])\n",
    "tree_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to find the prediction of the random forest, we can just take the average of these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.04"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tree_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is exactly the prediction of our random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.04])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.predict(X_test[:1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at the tree predictions, we'll see that we received different predictions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{31.0, 64.0, 77.0, 78.0, 96.0, 116.0, 144.0, 146.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tree_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how would we receive ten predictions if we are training each estimator on exactly the same data, and each estimator is following the same rules?  The short answer is, we can't.  If we train ten estimators on exactly the same data, each tree would wind up exactly the same, and each would fit thus fit to the random variation in the data in the same way.  \n",
    "\n",
    "Fitting the same highly variant tree multiple times and then taking the average doesn't help us reduce our error of variance.  Instead we would like ten different trees, and then take the average of these differing trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish this, we simply train our tree on different samples of the data.  This idea is just what it sounds like.  Each tree of a random forest only trains on a subset of the training data.  This way, even if all of the features were trained on, we would get different trees.  This is because each tree would see different training data.\n",
    "\n",
    "So with a random forest we not only train multiple trees, but train the trees on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we were introduced to a random forest.  We saw that we can initialize a random forest with a number of estimators, where each estimator is a decision tree that fits to our data.  To create variation in our decision trees, each decision tree is fit to a subset of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrapping** means that when we take a subsample the datset, we sample our data with replacement.  The advantage of this is that it increases the differences of the subsamples to train the trees.  First, let's understand how bootstrapping works, and then we can understand how it increases the variance of our subsamples, and thus our trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What it means to bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have observations one through thirteen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_to_thirteen = np.arange(1, 14)\n",
    "one_to_thirteen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were just to take a normal subsample of the data, then we would select five of the observations.  And each observation could only be selected once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 8, 2, 7, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(21)\n",
    "np.random.choice(one_to_thirteen, 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above, each of our selected datapoints is unique as they are removed from the pool of available data once selected.  When we sample with replacement, or bootstrap, this isn't the case.  We select from the list of observations, but do not remove the selected observation from the pool of data we select from.  We can bootstrap our array with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9,  5,  1,  1,  9,  4, 13])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(21)\n",
    "np.random.choice(one_to_thirteen, 8, replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we see that we can select the same number multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Bootstrapping to Our Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand the operation behind bootstrapping let's see what happens if we apply bootstrapping to our training sets.  We'll continue to use our list of numbers, one through thirteen, to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_to_thirteen = np.arange(1, 14)\n",
    "x_train, x_test = train_test_split(one_to_thirteen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, mimicking the procedure of a random forest, let's begin by taking subsamples of our data, without replacement.  Let's say that we select sixty percent of our training set each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  4,  6,  7,  9, 10],\n",
       "       [ 3,  4, 10, 11, 12, 13],\n",
       "       [ 3,  4,  6,  7, 11, 12],\n",
       "       [ 4,  7,  9, 10, 11, 12],\n",
       "       [ 3,  4,  6,  9, 10, 12]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "datasets = np.stack([np.random.choice(x_train, 6, replace = False) for num in range(0, 5)])\n",
    "datasets.sort(axis=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look closely at our datasets, notice that a couple of them are fairly similar.  The first and last rows share five of the six datapoints.  And the second and third rows share four of six datapoints.  This can be problematic because remember we want variation in our trees, as we want our trees to discover different patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens when we selecting subsamples of our training set with bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  3,  7, 11, 11, 13],\n",
       "       [ 6,  9, 10, 10, 11, 13],\n",
       "       [ 6,  7, 10, 12, 12, 13],\n",
       "       [ 4,  4,  6,  6,  7, 11],\n",
       "       [ 3,  9, 11, 12, 13, 13]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "datasets = np.stack([np.random.choice(x_train, 6, replace = True) for num in range(0, 5)])\n",
    "datasets.sort(axis=1)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we get a lot more variation in our data.  With bootstrapping, we simply have a lot more variations in our data.  So bootstrapping allows to essentially put random weights on various datapoints, which allows us to construct different trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping in Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our random forest in sklearn, bootstrapping is the default behavior of a random forest.  If we wanted to turn bootstrapping off, we can pass through an argument of `bootstrap = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor(bootstrap = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't want to do that.  \n",
    "\n",
    "With the current default behavior, our random forest will sample with replacement before constructing each tree in the random forest.  This increases the variances of our trees.  And this is a good thing, because we will ultimately accomodate for that variance by aggregate our trees at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Random Forest Top to Bottom](https://www.gormanalysis.com/blog/random-forest-from-top-to-bottom/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
